{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Tne47IIbRX"
      },
      "source": [
        "# CNN for FER\n",
        "\n",
        "By Fatemeh Ghezloo and Michelle Lin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXywwh4ZKiWS"
      },
      "source": [
        "## Usage\n",
        "\n",
        "The purpose of this notebook is to train a CNN for Facial Expression Recognition Task on FER2013 dataset. For improving the CNN performance, we have enabled the data augmentation to add more image samples to class disgust. This is done to make the dataset more balanced among different classes.\n",
        "\n",
        "To run this notebook, please make sure to first download the [FER2013 Kaggle dataset](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge). Because this dataset is part of a challenge, we are not allowed to share the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgtLRtHgMqAZ"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Run the following sections to make sure that you have imported all necessary packages, can read from/write to your Google Drive, and have copied all data to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0E0spHSzbiy",
        "outputId": "045058fe-3050-432d-ff76-675a25224980"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive/')\n",
        "!ls /gdrive\n",
        "\n",
        "import os\n",
        "\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/final_project/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "    # !wget https://courses.cs.washington.edu/courses/cse599g1/19au/files/homework2.tar\n",
        "    # !tar -xvf homework2.tar\n",
        "    # !rm homework2.tar\n",
        "DATA_PATH = BASE_PATH + 'data/'\n",
        "\n",
        "os.chdir(BASE_PATH)\n",
        "!pwd\n",
        "!ls\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import h5py\n",
        "import sys\n",
        "import pt_util\n",
        "sys.path.append(BASE_PATH)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive/\n",
            "MyDrive\n",
            "/gdrive/My Drive/colab_files/final_project\n",
            "1\t\t  disgust_training.npy\tneutral_training.npy  test\n",
            "2\t\t  fer2013.csv\t\tpt_util.py\t      tiny_imagenet\n",
            "data\t\t  logs\t\t\t__pycache__\n",
            "data.h5\t\t  logs_sad\t\tsad_test.npy\n",
            "disgust_test.npy  neutral_test.npy\tsad_training.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9POoB63zxtG"
      },
      "source": [
        "## Preprocess Input And Data Augmentation\n",
        "\n",
        "In this section, data is loaded from a csv file and saved as h5py. \n",
        "\n",
        "In order to use data augmentation feature, make sue variable ***augment*** is set to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuKph8rdzjXr",
        "outputId": "15df1b75-da89-4e21-ed5b-cbe3d2a568dc"
      },
      "source": [
        "# create data and label for FER2013\n",
        "# labels: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file = BASE_PATH + 'fer2013.csv'\n",
        "\n",
        "augment = False\n",
        "\n",
        "# Creat the list to store the data and label information\n",
        "y = []\n",
        "x = []\n",
        "\n",
        "datapath = os.path.join(BASE_PATH,'data.h5')\n",
        "if not os.path.exists(os.path.dirname(datapath)):\n",
        "    os.makedirs(os.path.dirname(datapath))\n",
        "\n",
        "with open(file,'r') as csvin:\n",
        "    data=csv.reader(csvin)\n",
        "    for row in data:\n",
        "      if row[-1] == 'Training' or row[-1] == \"PublicTest\" or row[-1] == 'PrivateTest':\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "            temp_list.append(int(pixel))\n",
        "        I = np.asarray(temp_list)\n",
        "        y.append(int(row[0]))\n",
        "        x.append(I.tolist())\n",
        "\n",
        "\n",
        "#Data Augmentation\n",
        "if augment:\n",
        "  augmented_data_path = BASE_PATH + 'logs/epoch_1200/disgust_new.npy'\n",
        "  new_disgust = np.load(augmented_data_path)\n",
        "  new_disgust = new_disgust.reshape(len(new_disgust), 2304)\n",
        "  I = new_disgust.tolist()\n",
        "\n",
        "  for row in I:\n",
        "    y.append(1)\n",
        "    x.append(row)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, train_size=0.66, shuffle = True, stratify=y)\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(X_test))\n",
        "\n",
        "datafile = h5py.File(datapath, 'w')\n",
        "\n",
        "datafile.create_dataset(\"Test_pixel\", dtype = 'uint8', data=X_test)\n",
        "datafile.create_dataset(\"Test_label\", dtype = 'int64', data=y_test)\n",
        "datafile.create_dataset(\"Train_pixel\", dtype = 'uint8', data=X_train)\n",
        "datafile.create_dataset(\"Train_label\", dtype = 'int64', data=y_train)\n",
        "datafile.close()\n",
        "\n",
        "print(\"Save data finish!!!\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23685, 2304)\n",
            "(11843, 2304)\n",
            "Save data finish!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u23ImG-b3jjE"
      },
      "source": [
        "##FER Dataset\n",
        "\n",
        "Run this to build FER dataset and it's dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFGchQAc3lGs"
      },
      "source": [
        "''' Fer2013 Dataset class'''\n",
        "\n",
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch.utils.data as data\n",
        "\n",
        "class FER2013(data.Dataset):\n",
        "\n",
        "    def __init__(self, split='Training', transform=None):\n",
        "        self.transform = transform\n",
        "        self.split = split  # training set or test set\n",
        "        self.data = h5py.File(BASE_PATH + 'data.h5', 'r', driver='core')\n",
        "        # now load the picked numpy arrays\n",
        "        if self.split == 'Training':\n",
        "            self.train_data = self.data['Train_pixel']\n",
        "            self.train_labels = self.data['Train_label']\n",
        "            self.train_data = np.asarray(self.train_data)\n",
        "            self.train_data = self.train_data.reshape((23685, 48, 48))\n",
        "\n",
        "        elif self.split == 'Test':\n",
        "            self.PublicTest_data = self.data['Test_pixel']\n",
        "            self.PublicTest_labels = self.data['Test_label']\n",
        "            self.PublicTest_data = np.asarray(self.PublicTest_data)\n",
        "            self.PublicTest_data = self.PublicTest_data.reshape((11843, 48, 48))\n",
        "\n",
        "        else:\n",
        "            self.PrivateTest_data = self.data['PrivateTest_pixel']\n",
        "            self.PrivateTest_labels = self.data['PrivateTest_label']\n",
        "            self.PrivateTest_data = np.asarray(self.PrivateTest_data)\n",
        "            self.PrivateTest_data = self.PrivateTest_data.reshape((3589, 48, 48))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.split == 'Training':\n",
        "            img, target = self.train_data[index], self.train_labels[index]\n",
        "        elif self.split == 'Test':\n",
        "            img, target = self.PublicTest_data[index], self.PublicTest_labels[index]\n",
        "        else:\n",
        "            img, target = self.PrivateTest_data[index], self.PrivateTest_labels[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = img[:, :, np.newaxis]\n",
        "        img = np.concatenate((img, img, img), axis=2)\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'Training':\n",
        "            return len(self.train_data)\n",
        "        elif self.split == 'Test':\n",
        "            return len(self.PublicTest_data)\n",
        "        else:\n",
        "            return len(self.PrivateTest_data)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BReHLxl-kxgz"
      },
      "source": [
        "##CNN Structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6s5Fzlv4P5Y"
      },
      "source": [
        "###CNN - Paper Structure\n",
        "\n",
        "Here we implemented the CNN structure from this [paper](https://arxiv.org/abs/1711.00648) that we replicated. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rtWJgRA4TO1"
      },
      "source": [
        "class FERCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(FERCNN, self).__init__()\n",
        "        \n",
        "        self.conv_layer = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),                      \n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "     \n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(6400, 256),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(256, 7)\n",
        "        )\n",
        " \n",
        "\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        print(x.shape)\n",
        "\n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R2TMznH4nhO"
      },
      "source": [
        "###CNN - VGG19\n",
        "\n",
        "We chose VGG19 model as a deeper network and In this section we implemented it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nh3defK4iCq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 7)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.dropout(out, p=0.5, training=self.training)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrjbdiMS4kVX"
      },
      "source": [
        "###CNN - Modified Paper Structure \n",
        "\n",
        "In this section we modified paper's structure by adding one more convolution layer to it. Also, to prevent overfitting, we added dropouts at the fully connection part.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrNoX75r4kw_"
      },
      "source": [
        "class FERCNNModified(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(FERCNNModified, self).__init__()\n",
        "        \n",
        "        self.conv_layer = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),                      \n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "     \n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(6400, 256),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Linear(256, 7)\n",
        "        )\n",
        " \n",
        "\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWXryaUe5oYv"
      },
      "source": [
        "##Train and Test\n",
        "\n",
        "Implementation of Train and Test functions. Also, this section provides total accuracy per epoch, accuracy per classes and loss values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dI9LUV45xnj"
      },
      "source": [
        "import time\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    class_correct = list(0. for i in range(7))\n",
        "    class_total = list(0. for i in range(7))\n",
        "    for batch_idx, (data, label) in enumerate(train_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        c = (predicted == label.data).squeeze()\n",
        "        listofints = [int(x) for x in label.shape]\n",
        "        for i in range(listofints[0]):\n",
        "            l = label.data[i]\n",
        "            class_correct[l] += c[i]\n",
        "            class_total[l] += 1\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                time.ctime(time.time()),\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        #     for i in range(7):\n",
        "        #       if class_total[i] == 0:\n",
        "        #             print('Accuracy of {} : {} / {} = {:.4f} %'.format(i, class_correct[i], class_total[i], 0))\n",
        "        #       else :\n",
        "        #             print('Accuracy of {} : {} / {} = {:.4f} %'.format(i, class_correct[i], class_total[i], 100 * class_correct[i].item() / class_total[i]))\n",
        "    \n",
        "    return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader, log_interval=None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    class_correct = list(0. for i in range(7))\n",
        "    class_total = list(0. for i in range(7))\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            test_loss_on = model.loss(output, label, reduction='sum').item()\n",
        "            test_loss += test_loss_on\n",
        "            pred = output.max(1)[1]\n",
        "            correct_mask = pred.eq(label.view_as(pred))\n",
        "            num_correct = correct_mask.sum().item()\n",
        "            correct += num_correct\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            c = (predicted == label.data).squeeze()\n",
        "\n",
        "            listofints = [int(x) for x in label.shape]\n",
        "            for i in range(listofints[0]):\n",
        "              l = label.data[i]\n",
        "              class_correct[l] += c[i]\n",
        "              class_total[l] += 1\n",
        "\n",
        "            if log_interval is not None and batch_idx % log_interval == 0:\n",
        "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    time.ctime(time.time()),\n",
        "                    batch_idx * len(data), len(test_loader.dataset),\n",
        "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "    print(\"Test acc:\")\n",
        "    for i in range(7):\n",
        "      \n",
        "      if class_total[i] == 0:\n",
        "         print('Accuracy of class {} : {} / {} = {:.4f} %'.format(i, class_correct[i], class_total[i], 0))\n",
        "      else :\n",
        "         print('Accuracy of class {} : {} / {} = {:.4f} %'.format(i, class_correct[i], class_total[i], 100 * class_correct[i].item() / class_total[i]))\n",
        "    \n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFi3qkaR6R0Q"
      },
      "source": [
        "##Data Augmentation using Transforms\n",
        "\n",
        "Adding transforms to train set to prevent our model from simply memorizing the input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL55rDws6SbL"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=10, contrast=5, saturation=0, hue=0),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "trainset = FER2013(split = 'Training', transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=1)\n",
        "PublicTestset = FER2013(split = 'Test', transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(PublicTestset, batch_size=32, shuffle=False, num_workers=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpZ3rDxj6soi"
      },
      "source": [
        "##Evaluation\n",
        "\n",
        "Training model for 500 epochs. Plotting train loss, test loss and test accuracy.\n",
        "\n",
        "Also you can choose among the three different network structure implemented in this notebook. Simply uncomment your desired model and comment the other two in the *choose model* section. We reported our results based on the modified version of the paper structue(i.e. model = FERCNNModified().to(device))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL5Yp79p6mAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "2cad172d-c754-40d8-aec2-deda29d59619"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 32\n",
        "EPOCHS = 500\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0.0005\n",
        "\n",
        "EXPERIMENT_VERSION = \"0.41\" # increment this to start a new experiment\n",
        "LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\n",
        "\n",
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
        "          'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "\n",
        "# Choose model\n",
        "# model = VGG('VGG19').to(device)\n",
        "# model = FERCNN().to(device)\n",
        "model = FERCNNModified().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, betas=(0.5,0.999))\n",
        "start_epoch = model.load_last_model(LOG_PATH)\n",
        "\n",
        "\n",
        "train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH + 'log.pkl', ([], [], []))\n",
        "test_loss, test_accuracy = test(model, device, test_loader)\n",
        "\n",
        "test_losses.append((start_epoch, test_loss))\n",
        "test_accuracies.append((start_epoch, test_accuracy))\n",
        "\n",
        "try:\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
        "        test_loss, test_accuracy = test(model, device, test_loader)\n",
        "        train_losses.append((epoch, train_loss))\n",
        "        test_losses.append((epoch, test_loss))\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        pt_util.write_log(LOG_PATH + 'log.pkl', (train_losses, test_losses, test_accuracies))\n",
        "        model.save_best_model(test_accuracy, LOG_PATH + '%03d.pt' % epoch)\n",
        "\n",
        "\n",
        "except KeyboardInterrupt as ke:\n",
        "    print('Interrupted')\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    model.save_model(LOG_PATH + '%03d.pt' % epoch, 0)\n",
        "    ep, val = zip(*train_losses)\n",
        "    pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error')\n",
        "    ep, val = zip(*test_losses)\n",
        "    pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error')\n",
        "    ep, val = zip(*test_accuracies)\n",
        "    pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n",
            "Test acc:\n",
            "Accuracy of class 0 : 0.0 / 1634.0 = 0.0000 %\n",
            "Accuracy of class 1 : 18.0 / 181.0 = 9.9448 %\n",
            "Accuracy of class 2 : 838.0 / 1690.0 = 49.5858 %\n",
            "Accuracy of class 3 : 0.0 / 2966.0 = 0.0000 %\n",
            "Accuracy of class 4 : 8.0 / 2005.0 = 0.3990 %\n",
            "Accuracy of class 5 : 541.0 / 1321.0 = 40.9538 %\n",
            "Accuracy of class 6 : 1.0 / 2046.0 = 0.0489 %\n",
            "\n",
            "Test set: Average loss: 1.9730, Accuracy: 1406/11843 (12%)\n",
            "\n",
            "Fri Dec 18 21:37:35 2020 Train Epoch: 0 [0/23685 (0%)]\tLoss: 1.966565\n",
            "Fri Dec 18 21:37:38 2020 Train Epoch: 0 [3200/23685 (13%)]\tLoss: 1.752955\n",
            "Fri Dec 18 21:37:41 2020 Train Epoch: 0 [6400/23685 (27%)]\tLoss: 1.862498\n",
            "Fri Dec 18 21:37:45 2020 Train Epoch: 0 [9600/23685 (40%)]\tLoss: 1.655198\n",
            "Interrupted\n",
            "Saved /gdrive/My Drive/colab_files/final_project/data/logs/0.41/000.pt\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fa3a43279338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%03d.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mpt_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    }
  ]
}