{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN_Keras_Version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoyju1EPHLhO"
      },
      "source": [
        "# CycleGAN for FER - Keras Version\n",
        "\n",
        "By Fatemeh Ghezloo and Michelle Lin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ASsUQbfTq3"
      },
      "source": [
        "## Usage\n",
        "The purpose of this notebook is to train a CycleGAN for Facial Expression Recognition Task on FER2013 dataset. We use this network to generate more sample images to supplement minority classes in our dataset. Generated images by this model are saved and then loaded as input to our CNN model for the classification task.\n",
        "\n",
        "To run this notebook, please make sure to first download the FER2013 Kaggle dataset. Because this dataset is part of a challenge, we are not allowed to share the data.\n",
        "\n",
        "\n",
        "Most of the code is implemented by us but it is based off of this [Github Repository](https://github.com/ivadym/FER).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEprTw6Qf0cJ"
      },
      "source": [
        "## Initial setup\n",
        "\n",
        "Run the following sections to make sure that you have imported all necessary packages, can read from/write to your Google Drive, and have copied all data to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv9IltNLViJ_",
        "outputId": "ac8a8d85-cd3a-42d5-d738-cc48a2911b53"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NExn82KTVjuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff2fe3f-819d-4a09-f84a-ec2fb0d8163a"
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/final_project/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = '/content/'\n",
        "\n",
        "if not os.path.exists(os.path.join(DATA_PATH, 'fer2013.csv')):\n",
        "    os.chdir(BASE_PATH)\n",
        "    # !cp fer2013.csv /content\n",
        "\n",
        "!ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fer2013.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V3ktSNh3c52"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from io import BytesIO\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dropout, Concatenate, Activation, UpSampling2D, Conv2D, LeakyReLU\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks9GF1wanM0P"
      },
      "source": [
        "## Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OehOUexujqhJ"
      },
      "source": [
        "from keras.engine import Layer, InputSpec\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras import backend as K\n",
        "\n",
        "class InstanceNormalization(Layer):\n",
        "    def __init__(self,\n",
        "                 axis=None,\n",
        "                 epsilon=1e-3,\n",
        "                 center=True,\n",
        "                 scale=True,\n",
        "                 beta_initializer='zeros',\n",
        "                 gamma_initializer='ones',\n",
        "                 beta_regularizer=None,\n",
        "                 gamma_regularizer=None,\n",
        "                 beta_constraint=None,\n",
        "                 gamma_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(InstanceNormalization, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        ndim = len(input_shape)\n",
        "        if self.axis == 0:\n",
        "            raise ValueError('Axis cannot be zero')\n",
        "\n",
        "        if (self.axis is not None) and (ndim == 2):\n",
        "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=ndim)\n",
        "\n",
        "        if self.axis is None:\n",
        "            shape = (1,)\n",
        "        else:\n",
        "            shape = (input_shape[self.axis],)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(shape=shape,\n",
        "                                         name='gamma',\n",
        "                                         initializer=self.gamma_initializer,\n",
        "                                         regularizer=self.gamma_regularizer,\n",
        "                                         constraint=self.gamma_constraint)\n",
        "        else:\n",
        "            self.gamma = None\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(shape=shape,\n",
        "                                        name='beta',\n",
        "                                        initializer=self.beta_initializer,\n",
        "                                        regularizer=self.beta_regularizer,\n",
        "                                        constraint=self.beta_constraint)\n",
        "        else:\n",
        "            self.beta = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        reduction_axes = list(range(0, len(input_shape)))\n",
        "\n",
        "        if (self.axis is not None):\n",
        "            del reduction_axes[self.axis]\n",
        "\n",
        "        del reduction_axes[0]\n",
        "\n",
        "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
        "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
        "        normed = (inputs - mean) / stddev\n",
        "\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        if self.axis is not None:\n",
        "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        if self.scale:\n",
        "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "            normed = normed * broadcast_gamma\n",
        "        if self.center:\n",
        "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "            normed = normed + broadcast_beta\n",
        "        return normed\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'axis': self.axis,\n",
        "            'epsilon': self.epsilon,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
        "        }\n",
        "        base_config = super(InstanceNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89mnFzTepEws"
      },
      "source": [
        "##Input Preprocessing\n",
        "\n",
        "In this section we split the fer2013.csv file into different npy files for training and testing sets of reference class(neutral) and target class (disgust and sad).\n",
        "\n",
        "Also, other functions to preprocess data and augment them by applying some transforms are provided for preventing the models to memorize the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-hE8L_2SQgX"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "\n",
        "emotionsToInd = {\"Angry\":0, \"Disgust\":1, \"Fear\":2, \"Happy\":3, \"Sad\":4, \"Surprise\":5, \"Neutral\":6}\n",
        "indToEmotions = {emotionsToInd[key]:key for key in emotionsToInd}\n",
        "setSizes = {\"Training\":[], \"PublicTest\":[], \"PrivateTest\":[]}\n",
        "file = BASE_PATH + 'fer2013.csv'\n",
        "\n",
        "neutral_training = open(BASE_PATH + 'neutral_training.npy', mode='wb')\n",
        "\n",
        "neutral_test = open(BASE_PATH + 'neutral_test.npy', mode='wb')\n",
        "\n",
        "disgust_training = open(BASE_PATH + 'disgust_training.npy', mode='wb')\n",
        "\n",
        "disgust_test = open(BASE_PATH + 'disgust_test.npy', mode='wb')\n",
        "\n",
        "sad_training = open(BASE_PATH + 'sad_training.npy', mode='wb')\n",
        "\n",
        "sad_test = open(BASE_PATH + 'sad_test.npy', mode='wb')\n",
        "\n",
        "I1 = []\n",
        "I2 = []\n",
        "I3 = []\n",
        "I4 = []\n",
        "I5 = []\n",
        "I6 = []\n",
        "\n",
        "with open(file,'r') as csvin:\n",
        "  data = csv.reader(csvin)\n",
        "  for row in data:\n",
        "    if row[-1] == 'Training':\n",
        "      if int(row[0]) == 6:\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "          temp_list.append(int(pixel))\n",
        "        I1.append(np.asarray(temp_list))\n",
        "      if int(row[0]) == 1:\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "          temp_list.append(int(pixel))\n",
        "        I2.append(np.asarray(temp_list))\n",
        "      if int(row[0]) == 4:\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "          temp_list.append(int(pixel))\n",
        "        I5.append(np.asarray(temp_list))\n",
        "\n",
        "    if row[-1] == 'PublicTest':\n",
        "      if int(row[0]) == 6:\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "          temp_list.append(int(pixel))\n",
        "        I3.append(np.asarray(temp_list))\n",
        "      if int(row[0]) == 1:\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "          temp_list.append(int(pixel))\n",
        "        I4.append(np.asarray(temp_list))\n",
        "      if int(row[0]) == 4:\n",
        "        temp_list = []\n",
        "        for pixel in row[1].split( ):\n",
        "          temp_list.append(int(pixel))\n",
        "        I6.append(np.asarray(temp_list))\n",
        "np.save(neutral_training, I1)\n",
        "np.save(disgust_training, I2)\n",
        "np.save(neutral_test, I3)\n",
        "np.save(disgust_test, I4)\n",
        "np.save(sad_training, I5)\n",
        "np.save(sad_test, I6)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRhOKWZIauP-"
      },
      "source": [
        "# PARAMETERS \n",
        "\n",
        "# Input shape\n",
        "img_height, img_width = 48, 48 # FER-2013\n",
        "channels    = 1\n",
        "img_shape   = (img_height, img_width, channels)\n",
        "\n",
        "# Loss weights\n",
        "lambda_cycle    = 10.0  # Cycle-consistency loss\n",
        "\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "# Training\n",
        "epochs          = 30000\n",
        "batch_size      = 1\n",
        "save_interval   = 50\n",
        "\n",
        "# DATASETS \n",
        "# Folder where logs and models are stored\n",
        "folder = BASE_PATH + \"logs\"\n",
        "\n",
        "# Data paths\n",
        "dataset_A = BASE_PATH + \"neutral_training.npy\"\n",
        "dataset_B = BASE_PATH + \"disgust_training.npy\"\n",
        "\n",
        "image_A = BASE_PATH + \"neutral_test.npy\"\n",
        "image_B = BASE_PATH + \"disgust_test.npy\"\n",
        "\n",
        "\n",
        "# DATA PREPARATION \n",
        "def preprocess_input(x): \n",
        "    x = x/127.5\n",
        "    x = x-1\n",
        "    return x\n",
        "\n",
        "# Function that reads the images from the csv file\n",
        "def load_data(dataset):\n",
        "    loaded_images = np.load(dataset)\n",
        "    loaded_images = loaded_images.reshape((len(loaded_images), img_height, img_width,1))\n",
        "    images = preprocess_input(loaded_images)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def augmented_data(loaded_images):\n",
        "    images = np.empty((len(loaded_images), img_height, img_width, channels))\n",
        "    i = 0\n",
        "    for single_image in loaded_images:\n",
        "        if np.random.random() > 0.5:\n",
        "            single_image = np.fliplr(single_image)\n",
        "\n",
        "        images[i, :, :, :] = single_image\n",
        "        i += 1\n",
        "\n",
        "    shuffled_indexes = np.random.permutation(len(images))\n",
        "    images = images[shuffled_indexes]\n",
        "\n",
        "    return images"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTPEe464FxwN"
      },
      "source": [
        "##Generator and Discriminator\n",
        "\n",
        "Generator and Discriminator network structures are defined in this section.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NSZTw6t29ND"
      },
      "source": [
        "# GENERATOR\n",
        "def generator():\n",
        "    \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "    def conv_2d_v0(layer_input, filters, f_size = 4):\n",
        "        \"\"\"Layers used during downsampling\"\"\"\n",
        "        d = Conv2D(filters, kernel_size = f_size, strides = 2, padding = \"same\")(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        d = InstanceNormalization()(d)\n",
        "        return d\n",
        "\n",
        "    def deconv_2d_v0(layer_input, skip_input, filters, f_size = 4, dropout_rate = 0):\n",
        "        \"\"\"Layers used during upsampling\"\"\"\n",
        "        u = UpSampling2D(size = 2)(layer_input)\n",
        "        u = Conv2D(filters, kernel_size = f_size, strides = 1, padding = \"same\", activation = \"relu\")(u)\n",
        "        if dropout_rate:\n",
        "            u = Dropout(dropout_rate)(u)\n",
        "        u = InstanceNormalization()(u)\n",
        "        u = Concatenate()([u, skip_input])\n",
        "        return u\n",
        "\n",
        "    # Image input\n",
        "    d0 = Input(shape = img_shape)\n",
        "\n",
        "    # Downsampling\n",
        "    d1 = conv_2d_v0(d0, 32)\n",
        "    d2 = conv_2d_v0(d1, 32*2)\n",
        "    d3 = conv_2d_v0(d2, 32*4)\n",
        "    d4 = conv_2d_v0(d3, 32*8)\n",
        "\n",
        "    # Upsampling\n",
        "    u1 = deconv_2d_v0(d4, d3, 32*4)\n",
        "    u2 = deconv_2d_v0(u1, d2, 32*2)\n",
        "    u3 = deconv_2d_v0(u2, d1, 32)\n",
        "\n",
        "    u4 = UpSampling2D(size = 2)(u3)\n",
        "    output_img = Conv2D(channels, kernel_size = 4, strides = 1, padding = \"same\", activation = \"tanh\")(u4)\n",
        "\n",
        "    return Model(d0, output_img)\n",
        "\n",
        "# DISCRIMINATOR \n",
        "\n",
        "def discrimiantor():\n",
        "    \n",
        "    def discriminator_layer(layer_input, filters, f_size = 4, normalization = True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size = f_size, strides = 2, padding = \"same\")(layer_input)\n",
        "        d = LeakyReLU(alpha = 0.2)(d)\n",
        "        if normalization:\n",
        "            d = InstanceNormalization()(d)\n",
        "        return d\n",
        "\n",
        "    img = Input(shape = img_shape)\n",
        "\n",
        "    d1 = discriminator_layer(img, 64, normalization = False)\n",
        "    d2 = discriminator_layer(d1, 64*2)\n",
        "    d3 = discriminator_layer(d2, 64*4)\n",
        "    d4 = discriminator_layer(d3, 64*8)\n",
        "\n",
        "    validity = Conv2D(1, kernel_size = 4, strides = 1, padding = \"same\")(d4)\n",
        "\n",
        "    return Model(img, validity)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FQX00hYF5lV"
      },
      "source": [
        "##Building and Compiling\n",
        "\n",
        "This section provides the code to define, build and compile two generator models and two discriminator models and loss functions of the model.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cawJixVt3LIY"
      },
      "source": [
        "# Build and compile the discriminators\n",
        "d_A = discrimiantor()\n",
        "d_B = discrimiantor()\n",
        "d_A.compile(\n",
        "\tloss \t\t= \"mse\",\n",
        "    optimizer \t= optimizer,\n",
        "    metrics \t= [\"accuracy\"])\n",
        "d_B.compile(\n",
        "\tloss \t\t= \"mse\",\n",
        "    optimizer \t= optimizer,\n",
        "    metrics \t= [\"accuracy\"])\n",
        "\n",
        "# Build and compile the generators\n",
        "g_AB = generator()\n",
        "g_BA = generator()\n",
        "g_AB.compile(\n",
        "\tloss \t\t= \"binary_crossentropy\",\n",
        "\toptimizer \t= optimizer)\n",
        "g_BA.compile(\n",
        "\tloss \t\t=\"binary_crossentropy\",\n",
        "\toptimizer \t= optimizer)\n",
        "\n",
        "# Input images from both domains\n",
        "img_A = Input(shape = img_shape)\n",
        "img_B = Input(shape = img_shape)\n",
        "\n",
        "# Translate images to the other domain\n",
        "fake_B = g_AB(img_A)\n",
        "fake_A = g_BA(img_B)\n",
        "\n",
        "# Translate images back to original domain\n",
        "reconstr_A = g_BA(fake_B)\n",
        "reconstr_B = g_AB(fake_A)\n",
        "\n",
        "# For the combined model we will only train the generators (updates discriminators weights only when discriminator.fit() is called but not when gan.fit() is called)\n",
        "d_A.trainable = False\n",
        "d_B.trainable = False\n",
        "\n",
        "# Discriminators determines validity of translated images\n",
        "valid_A = d_A(fake_A)\n",
        "valid_B = d_B(fake_B)\n",
        "combined = Model(\n",
        "    [img_A, img_B],\n",
        "    [valid_A, valid_B, fake_B, fake_A, reconstr_A, reconstr_B])\n",
        "combined.compile(\n",
        "    loss \t\t\t= [\"mse\", \"mse\", \"mae\", \"mae\", \"mae\", \"mae\"],\n",
        "    loss_weights\t= [1, 1, 1, 1, lambda_cycle, lambda_cycle], optimizer = optimizer)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pnWUMRvGOAa"
      },
      "source": [
        "##Save Generated Images\n",
        "\n",
        "Every 10 epoch, we test our model, and save the generated images. Here, a function is provided to save newly generated images. This also writes image pixels to a npy file to be loaded for CNN later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhBYVBc_3R4Y"
      },
      "source": [
        "from skimage import img_as_ubyte\n",
        "\n",
        "def check_file(filepath):\n",
        "    if not os.path.exists(os.path.dirname(filepath)):\n",
        "        os.makedirs(os.path.dirname(filepath))\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    I = []\n",
        "    imges_A = load_data(image_A)\n",
        "    imges_B = load_data(image_B)\n",
        "\n",
        "    check_file(folder + \"/epoch_%s/\" % (epoch))\n",
        "    filepath = folder + \"/epoch_%s/disgust_new.npy\" % (epoch)\n",
        "    disgust_new = open(filepath, mode='wb')\n",
        "    \n",
        "    titles = [\"Original_A\", \"Translated_A\", \"Reconstructed_A\", \"Original_B\", \"Translated_B\", \"Reconstructed_B\"]\n",
        "    n_iterations = min(len(imges_A), len(imges_B)) // 1\n",
        "    idx = 0\n",
        "    for it in range (0, n_iterations):\n",
        "      imgs_A = imges_A[idx : idx + 1]\n",
        "      imgs_B = imges_B[idx : idx + 1]\n",
        "\n",
        "    # Translate images to the other domain\n",
        "      fake_B = g_AB.predict(imgs_A)\n",
        "      fake_A = g_BA.predict(imgs_B)\n",
        "    # Translate back to original domain\n",
        "      reconstr_A = g_BA.predict(fake_B)\n",
        "      reconstr_B = g_AB.predict(fake_A)\n",
        "\n",
        "      gen_imgs = [np.squeeze(imgs_A, axis = 0), np.squeeze(fake_B, axis = 0), np.squeeze(reconstr_A, axis = 0),\n",
        "        np.squeeze(imgs_B, axis = 0), np.squeeze(fake_A, axis = 0), np.squeeze(reconstr_B, axis = 0)]\n",
        "\n",
        "      I.append(gen_imgs[1])\n",
        "      \n",
        "\n",
        "\n",
        "      for i in range(0, len(gen_imgs)):\n",
        "          imageio.imwrite(folder + \"/epoch_%s/%d_%s.png\" % (epoch, idx, titles[i]), img_as_ubyte(gen_imgs[i]))\n",
        "\n",
        "      idx = idx + 1\n",
        "\n",
        "    np.save(disgust_new,I)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8U4e03BGXqt"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here, input data is loaded, augmented, and then fed to networks to be trained. Every 10 epochs, all the losses and outputs of the model is saved using the save function provided earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq43JGUfjAJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe5bf47-4262-4e35-87a9-f65ce30773a0"
      },
      "source": [
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# Sample a batch of images from both domains\n",
        "imgs_A = load_data(dataset_A)\n",
        "imgs_B = load_data(dataset_B)\n",
        "\n",
        "discriminator_loss = []\n",
        "generator_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    imgs_A = augmented_data(imgs_A)\n",
        "    imgs_B = augmented_data(imgs_B)\n",
        "\n",
        "\n",
        "    # Translate images to opposite domain\n",
        "    fake_B = g_AB.predict(imgs_A)\n",
        "    fake_A = g_BA.predict(imgs_B)\n",
        "\n",
        "    # The generators want the discriminators to label the translated images as real\n",
        "    valid = np.ones((batch_size,) + (3, 3, 1))\n",
        "    fake = np.zeros((batch_size,) + (3, 3, 1))\n",
        "\n",
        "    n_iterations = min(len(imgs_A), len(imgs_B)) // batch_size\n",
        "    i = 0\n",
        "    for it in range (0, n_iterations):\n",
        "        imgs_A_p = imgs_A[i : i+batch_size]\n",
        "        fake_A_p = fake_A[i : i+batch_size]\n",
        "        imgs_B_p = imgs_B[i : i+batch_size]\n",
        "        fake_B_p = fake_B[i : i+batch_size]\n",
        "\n",
        "        # TRAIN DISCRIMINATORS \n",
        "        dA_loss_real = d_A.train_on_batch(imgs_A_p, valid)    # original images = real\n",
        "        dA_loss_fake = d_A.train_on_batch(fake_A_p, fake)     # translated images = Fake\n",
        "        \n",
        "        dB_loss_real = d_B.train_on_batch(imgs_B_p, valid)\n",
        "        dB_loss_fake = d_B.train_on_batch(fake_B_p, fake)\n",
        "\n",
        "        # TRAIN GENERATORS\n",
        "        g_loss = combined.train_on_batch([imgs_A_p, imgs_B_p], [valid, valid, imgs_A_p, imgs_B_p, imgs_A_p, imgs_B_p])\n",
        "\n",
        "        i += batch_size\n",
        "\n",
        "    # Disciminator losses\n",
        "    dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "    dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "    d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
        "\n",
        "    elapsed_time = datetime.datetime.now() - start_time\n",
        "    \n",
        "    # Plot the progress\n",
        "    print (\"Epoch: %d Time: %s \\n D-Loss %s \\n G-Loss %s \" % (epoch, elapsed_time, d_loss, g_loss))\n",
        "\n",
        "    # Register the losses\n",
        "    discriminator_loss.append(d_loss)\n",
        "    generator_loss.append(g_loss)\n",
        "\n",
        "    # If at save interval => save generated image samples\n",
        "    if epoch % save_interval == 0:\n",
        "        save_imgs(epoch)\n",
        "\n",
        "        np.save(folder + \"/epoch_%s/d_loss_%s.npy\" % (epoch, epoch), discriminator_loss)\n",
        "        np.save(folder + \"/epoch_%s/g_loss_%s.npy\" % (epoch, epoch), generator_loss)\n",
        "\n",
        "        d_A.save_weights(folder + \"/epoch_%s/weights_d_A_%s.h5\" % (epoch, epoch))\n",
        "        d_B.save_weights(folder + \"/epoch_%s/weights_d_B_%s.h5\" % (epoch, epoch))\n",
        "        g_AB.save_weights(folder + \"/epoch_%s/weights_g_AB_%s.h5\" % (epoch, epoch))\n",
        "        g_BA.save_weights(folder + \"/epoch_%s/weights_g_BA_%s.h5\" % (epoch, epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Time: 0:00:51.616839 \n",
            " D-Loss [0.01600469 1.        ] \n",
            " G-Loss [5.38951301574707, 0.6061256527900696, 0.015671439468860626, 0.9643536806106567, 0.719403862953186, 0.11765946447849274, 0.19073641300201416] \n",
            "Epoch: 1 Time: 0:03:22.308271 \n",
            " D-Loss [0.11641598 1.        ] \n",
            " G-Loss [4.914385795593262, 0.07189420610666275, 0.4109202027320862, 0.7062171101570129, 0.7726929187774658, 0.15506401658058167, 0.1402021199464798] \n",
            "Epoch: 2 Time: 0:03:56.972067 \n",
            " D-Loss [0.02078542 1.        ] \n",
            " G-Loss [5.050339698791504, 0.007401918526738882, 0.0850697010755539, 1.0164365768432617, 0.6182152628898621, 0.19990263879299164, 0.1324189454317093] \n",
            "Epoch: 3 Time: 0:04:32.047419 \n",
            " D-Loss [0.02326371 1.        ] \n",
            " G-Loss [4.114402770996094, 0.039583899080753326, 0.011753393337130547, 0.5270629525184631, 0.5846845507621765, 0.19258248805999756, 0.10254929214715958] \n",
            "Epoch: 4 Time: 0:05:06.598413 \n",
            " D-Loss [0.01210884 1.        ] \n",
            " G-Loss [3.473181962966919, 0.007980958558619022, 0.10129909217357635, 0.7186907529830933, 0.6618286967277527, 0.10523428022861481, 0.09310398250818253] \n",
            "Epoch: 5 Time: 0:05:41.469013 \n",
            " D-Loss [0.09142575 0.8888889 ] \n",
            " G-Loss [3.5601234436035156, 0.07750730961561203, 0.01749449223279953, 0.7150142788887024, 0.6766415238380432, 0.1064460426568985, 0.10090053826570511] \n",
            "Epoch: 6 Time: 0:06:16.726533 \n",
            " D-Loss [0.019039 1.      ] \n",
            " G-Loss [5.023367881774902, 0.0042981659062206745, 0.0027908766642212868, 0.8407174944877625, 0.8581652641296387, 0.18174439668655396, 0.14999517798423767] \n",
            "Epoch: 7 Time: 0:06:51.805595 \n",
            " D-Loss [0.05099698 1.        ] \n",
            " G-Loss [3.3826446533203125, 0.00742681510746479, 0.014489703811705112, 0.7556260824203491, 0.6617922782897949, 0.070682592689991, 0.12364839017391205] \n",
            "Epoch: 8 Time: 0:07:26.839046 \n",
            " D-Loss [0.01422166 1.        ] \n",
            " G-Loss [3.03694486618042, 0.0016330867074429989, 0.020342020317912102, 0.6931913495063782, 0.6030364036560059, 0.06743407249450684, 0.10444015264511108] \n",
            "Epoch: 9 Time: 0:08:01.733775 \n",
            " D-Loss [0.04145444 1.        ] \n",
            " G-Loss [3.0973212718963623, 0.011731375940144062, 0.009112250991165638, 0.8644427061080933, 0.8729485869407654, 0.08106142282485962, 0.05284722521901131] \n",
            "Epoch: 10 Time: 0:08:36.654919 \n",
            " D-Loss [0.01711949 1.        ] \n",
            " G-Loss [4.212696075439453, 0.005329082254320383, 0.020419225096702576, 0.5609337687492371, 0.9114108681678772, 0.14042621850967407, 0.13103407621383667] \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}